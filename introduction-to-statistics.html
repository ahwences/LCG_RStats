<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Introduction to Statistics | Probability and Statistics Classes</title>
  <meta name="description" content="4 Introduction to Statistics | Probability and Statistics Classes" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Introduction to Statistics | Probability and Statistics Classes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Introduction to Statistics | Probability and Statistics Classes" />
  
  
  

<meta name="author" content="Alejandro HernÃ¡ndez Wences" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="common-distributions.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html"><i class="fa fa-check"></i><b>2</b> Probability Spaces and Random Variables</a><ul>
<li class="chapter" data-level="2.1" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#probability-spaces"><i class="fa fa-check"></i><b>2.1</b> Probability Spaces</a><ul>
<li class="chapter" data-level="2.1.1" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#definition"><i class="fa fa-check"></i><b>2.1.1</b> Definition</a></li>
<li class="chapter" data-level="2.1.2" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#first-examples"><i class="fa fa-check"></i><b>2.1.2</b> First Examples</a></li>
<li class="chapter" data-level="2.1.3" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#continuousDiscreteDists"><i class="fa fa-check"></i><b>2.1.3</b> Important Examples</a></li>
<li class="chapter" data-level="2.1.4" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#properties-of-probability-measures"><i class="fa fa-check"></i><b>2.1.4</b> Properties of Probability Measures</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#random-elements"><i class="fa fa-check"></i><b>2.2</b> Random Elements</a><ul>
<li class="chapter" data-level="2.2.1" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#single-random-variables"><i class="fa fa-check"></i><b>2.2.1</b> Single Random Variables</a></li>
<li class="chapter" data-level="2.2.2" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#random-vectors"><i class="fa fa-check"></i><b>2.2.2</b> Random Vectors</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#conditional-probability-and-independence"><i class="fa fa-check"></i><b>2.3</b> Conditional Probability and Independence</a><ul>
<li class="chapter" data-level="2.3.1" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#intuition"><i class="fa fa-check"></i><b>2.3.1</b> Intuition</a></li>
<li class="chapter" data-level="2.3.2" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#discrete-case"><i class="fa fa-check"></i><b>2.3.2</b> Discrete case</a></li>
<li class="chapter" data-level="2.3.3" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#continuous-case"><i class="fa fa-check"></i><b>2.3.3</b> Continuous Case</a></li>
<li class="chapter" data-level="2.3.4" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#independence"><i class="fa fa-check"></i><b>2.3.4</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#cumulative-distribution-functions-cdfs"><i class="fa fa-check"></i><b>2.4</b> Cumulative Distribution Functions (CDFs)</a><ul>
<li class="chapter" data-level="2.4.1" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#single-random-variables-1"><i class="fa fa-check"></i><b>2.4.1</b> Single Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#expectations-and-moments"><i class="fa fa-check"></i><b>2.5</b> Expectations and Moments</a><ul>
<li class="chapter" data-level="2.5.1" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#single-random-variables-2"><i class="fa fa-check"></i><b>2.5.1</b> Single Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="probability-spaces-and-random-variables.html"><a href="probability-spaces-and-random-variables.html#random-vectors-1"><i class="fa fa-check"></i><b>2.5.2</b> Random Vectors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="common-distributions.html"><a href="common-distributions.html"><i class="fa fa-check"></i><b>3</b> Common Distributions</a><ul>
<li class="chapter" data-level="3.1" data-path="common-distributions.html"><a href="common-distributions.html#bernoulli"><i class="fa fa-check"></i><b>3.1</b> Bernoulli</a><ul>
<li class="chapter" data-level="3.1.1" data-path="common-distributions.html"><a href="common-distributions.html#definition-1"><i class="fa fa-check"></i><b>3.1.1</b> Definition</a></li>
<li class="chapter" data-level="3.1.2" data-path="common-distributions.html"><a href="common-distributions.html#model"><i class="fa fa-check"></i><b>3.1.2</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="common-distributions.html"><a href="common-distributions.html#geometric-distribution"><i class="fa fa-check"></i><b>3.2</b> Geometric Distribution</a><ul>
<li class="chapter" data-level="3.2.1" data-path="common-distributions.html"><a href="common-distributions.html#definition-2"><i class="fa fa-check"></i><b>3.2.1</b> Definition</a></li>
<li class="chapter" data-level="3.2.2" data-path="common-distributions.html"><a href="common-distributions.html#model-1"><i class="fa fa-check"></i><b>3.2.2</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="common-distributions.html"><a href="common-distributions.html#exponential-distribution"><i class="fa fa-check"></i><b>3.3</b> Exponential Distribution</a><ul>
<li class="chapter" data-level="3.3.1" data-path="common-distributions.html"><a href="common-distributions.html#definition-3"><i class="fa fa-check"></i><b>3.3.1</b> Definition</a></li>
<li class="chapter" data-level="3.3.2" data-path="common-distributions.html"><a href="common-distributions.html#model-2"><i class="fa fa-check"></i><b>3.3.2</b> Model</a></li>
<li class="chapter" data-level="3.3.3" data-path="common-distributions.html"><a href="common-distributions.html#important-properties"><i class="fa fa-check"></i><b>3.3.3</b> Important properties</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="common-distributions.html"><a href="common-distributions.html#binomial-distribution"><i class="fa fa-check"></i><b>3.4</b> Binomial Distribution</a><ul>
<li class="chapter" data-level="3.4.1" data-path="common-distributions.html"><a href="common-distributions.html#definition-4"><i class="fa fa-check"></i><b>3.4.1</b> Definition</a></li>
<li class="chapter" data-level="3.4.2" data-path="common-distributions.html"><a href="common-distributions.html#model-3"><i class="fa fa-check"></i><b>3.4.2</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="common-distributions.html"><a href="common-distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>3.5</b> Poisson Distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="common-distributions.html"><a href="common-distributions.html#definition-5"><i class="fa fa-check"></i><b>3.5.1</b> Definition</a></li>
<li class="chapter" data-level="3.5.2" data-path="common-distributions.html"><a href="common-distributions.html#model-4"><i class="fa fa-check"></i><b>3.5.2</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="common-distributions.html"><a href="common-distributions.html#gamma-distribution"><i class="fa fa-check"></i><b>3.6</b> Gamma Distribution</a><ul>
<li class="chapter" data-level="3.6.1" data-path="common-distributions.html"><a href="common-distributions.html#definition-6"><i class="fa fa-check"></i><b>3.6.1</b> Definition</a></li>
<li class="chapter" data-level="3.6.2" data-path="common-distributions.html"><a href="common-distributions.html#model-5"><i class="fa fa-check"></i><b>3.6.2</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="common-distributions.html"><a href="common-distributions.html#beta-distribution"><i class="fa fa-check"></i><b>3.7</b> Beta Distribution</a><ul>
<li class="chapter" data-level="3.7.1" data-path="common-distributions.html"><a href="common-distributions.html#model-6"><i class="fa fa-check"></i><b>3.7.1</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="common-distributions.html"><a href="common-distributions.html#normalDistribution"><i class="fa fa-check"></i><b>3.8</b> Normal Distribution</a><ul>
<li class="chapter" data-level="3.8.1" data-path="common-distributions.html"><a href="common-distributions.html#definition-7"><i class="fa fa-check"></i><b>3.8.1</b> Definition</a></li>
<li class="chapter" data-level="3.8.2" data-path="common-distributions.html"><a href="common-distributions.html#model-7"><i class="fa fa-check"></i><b>3.8.2</b> Model</a></li>
<li class="chapter" data-level="3.8.3" data-path="common-distributions.html"><a href="common-distributions.html#important-properties-1"><i class="fa fa-check"></i><b>3.8.3</b> Important properties</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="common-distributions.html"><a href="common-distributions.html#students-t-distribution"><i class="fa fa-check"></i><b>3.9</b> Studentâs t-distribution</a><ul>
<li class="chapter" data-level="3.9.1" data-path="common-distributions.html"><a href="common-distributions.html#definition-8"><i class="fa fa-check"></i><b>3.9.1</b> Definition</a></li>
<li class="chapter" data-level="3.9.2" data-path="common-distributions.html"><a href="common-distributions.html#model-8"><i class="fa fa-check"></i><b>3.9.2</b> Model</a></li>
<li class="chapter" data-level="3.9.3" data-path="common-distributions.html"><a href="common-distributions.html#cauchy-vs-normal"><i class="fa fa-check"></i><b>3.9.3</b> Cauchy VS Normal</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introduction-to-statistics.html"><a href="introduction-to-statistics.html"><i class="fa fa-check"></i><b>4</b> Introduction to Statistics</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction-to-statistics.html"><a href="introduction-to-statistics.html#bigpicture"><i class="fa fa-check"></i><b>4.1</b> Big Picture</a><ul>
<li class="chapter" data-level="4.1.1" data-path="introduction-to-statistics.html"><a href="introduction-to-statistics.html#data"><i class="fa fa-check"></i><b>4.1.1</b> Data</a></li>
<li class="chapter" data-level="4.1.2" data-path="introduction-to-statistics.html"><a href="introduction-to-statistics.html#models"><i class="fa fa-check"></i><b>4.1.2</b> Models</a></li>
<li class="chapter" data-level="4.1.3" data-path="introduction-to-statistics.html"><a href="introduction-to-statistics.html#probability"><i class="fa fa-check"></i><b>4.1.3</b> Probability</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="introduction-to-statistics.html"><a href="introduction-to-statistics.html#likelihood-function"><i class="fa fa-check"></i><b>4.2</b> Likelihood Function</a><ul>
<li class="chapter" data-level="4.2.1" data-path="introduction-to-statistics.html"><a href="introduction-to-statistics.html#definition-9"><i class="fa fa-check"></i><b>4.2.1</b> Definition</a></li>
<li class="chapter" data-level="4.2.2" data-path="introduction-to-statistics.html"><a href="introduction-to-statistics.html#maximum-likelihood-estimator-mle-point-estimate"><i class="fa fa-check"></i><b>4.2.2</b> Maximum Likelihood Estimator MLE (Point Estimate)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="introduction-to-statistics.html"><a href="introduction-to-statistics.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.3</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="4.3.1" data-path="introduction-to-statistics.html"><a href="introduction-to-statistics.html#definition-and-evaluation"><i class="fa fa-check"></i><b>4.3.1</b> Definition and Evaluation</a></li>
<li class="chapter" data-level="4.3.2" data-path="introduction-to-statistics.html"><a href="introduction-to-statistics.html#likelihood-ratio-tests-lrts"><i class="fa fa-check"></i><b>4.3.2</b> Likelihood Ratio Tests (LRTs)</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Probability and Statistics Classes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-statistics" class="section level1">
<h1><span class="header-section-number">4</span> Introduction to Statistics</h1>
<div id="bigpicture" class="section level2">
<h2><span class="header-section-number">4.1</span> Big Picture</h2>
<div id="data" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Data</h3>
<ul>
<li>Random variables live in the theoretical (mathematical) world</li>
<li>We <strong>assume</strong> that the variability of the data is adequately consistent
with the variability that <strong>would</strong> occur in a random sample from a given
distribution/random process.
<ul>
<li>Data itself is assumed to be random variables so that we can focus
on the best way to <strong>reason from random variables to inferences
about (theoretical) parameters</strong>.</li>
</ul></li>
<li>Beware, often the language of statistics takes a convenient shortcut by blurring the distinction between data and random variables.</li>
</ul>
</div>
<div id="models" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Models</h3>
<ul>
<li>Incorporate theoretical assumptions and subjunctive statements (of the form if â¦ then â¦).</li>
<li>Inference is based on what <strong>would</strong> happen if the data <strong>were</strong> to be random
variables distributed according to the statistical model.</li>
<li>The modelling assumption would be reasonable if the model were to <strong>describe accurately</strong> the variation in the data.</li>
<li>Typically one considers real observed data <span class="math inline">\(\bar{x}=(x_1,\dots,x_n)\)</span> on the one hand, and random vectors
<span class="math inline">\((X_1,\dots,X_n)\)</span> whose distribution depends on a parameter <span class="math inline">\(\theta\)</span> on the other. Then one assumes that the observed
data <span class="math inline">\(\bar{x}\)</span> is actually a realization of the random vector <span class="math inline">\(\bar{X}\)</span>, i.e one assumes that we have observed
the event
<span class="math inline">\(X_1=x_1,\dots,X_n=x_n\)</span> thus effectively considering the observed data <span class="math inline">\(\bar{x}\)</span> as random variables.</li>
</ul>
</div>
<div id="probability" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Probability</h3>
<ul>
<li><strong>Aleatoric Probability:</strong> The use of probability to describe <strong>variation</strong>. E.g.
âThe probability of rolling a 3 in a fair dice is 1/6â.</li>
<li><strong>Epistemic Probability:</strong> The use of probability to express <strong>knowledge</strong> (or beliefs). E.g. âIâm 90% sure that the capital of MÃ©xico is CDMXâ.</li>
</ul>
</div>
</div>
<div id="likelihood-function" class="section level2">
<h2><span class="header-section-number">4.2</span> Likelihood Function</h2>
<div id="definition-9" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Definition</h3>

<div class="definition">
<span id="def:unnamed-chunk-82" class="definition"><strong>Definition 4.1  </strong></span>
Let <span class="math inline">\(\{\mathcal{P}_\theta\}_{\theta\in\Theta}\)</span>
be a set of probability models (e.g.Â distributions)
indexed by a (possibly a vector) parameter <span class="math inline">\(\theta\)</span> that lives
in some space <span class="math inline">\(\Theta\)</span> (e.g.Â <span class="math inline">\(\mathbb{R}^n\)</span>).
</div>

<ul>
<li><p>Assume that, for each
<span class="math inline">\(\theta\)</span>, if <span class="math inline">\(\bar{X}=(X_1,\dots,X_n)\)</span> is a random vector whose
distribution is given by <span class="math inline">\(\mathcal{P}_{\theta}\)</span>, then <span class="math inline">\(\bar{X}\)</span> has density
<span class="math inline">\(f(\bar{x}\vert \theta)\)</span>.</p></li>
<li><p>Fix <span class="math inline">\(\bar{x}=(x_1,\dots,x_n)\)</span>.</p></li>
</ul>
<p>The <strong>likelihood function</strong> <span class="math inline">\(L(\bar{x}\vert \theta)\)</span> based on <span class="math inline">\(\bar{x}\)</span> is the function
of <span class="math inline">\(\theta\)</span> given by
<span class="math display">\[L(\bar{x} \vert \theta) = f(\bar{x}\vert \theta).\]</span></p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> Note that if we regard
<span class="math display">\[
L(\bar{x} \vert \theta) = f(\bar{x}\vert \theta)
\]</span>
as a function of <span class="math inline">\(\bar{x}\)</span> (with <span class="math inline">\(\theta\)</span> fixed) instead, then what we have is simply
the density function of <span class="math inline">\(\bar{x}\)</span> under the model <span class="math inline">\(\mathcal{P}_\theta\)</span>.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-84" class="example"><strong>Example 4.1  (i.i.d Gamma)  </strong></span>Consider the family of probability models for a random vector <span class="math inline">\(\bar{X}\)</span>
of size <span class="math inline">\(n\)</span> whose entries are all independent and identically distributed
<span class="math inline">\(Gamma(\alpha,\beta)\)</span>. Then for any vector <span class="math inline">\(\bar{x}=(x_1,\dots,x_n)\)</span>, and
using the independence assumption for the first equality, we have
<span class="math display">\[
\begin{aligned}
L(\bar{x}\vert \alpha,\beta) &amp;= \prod_{i=1}^n L(x_i\vert \alpha,\beta) \\
&amp;= \left(\frac{\beta^\alpha}{\Gamma(\alpha)}\right)^n \prod_{i=1}^n x_i^{\alpha-1}e^{-\beta x_i}\\
&amp;=  \left(\frac{\beta^\alpha}{\Gamma(\alpha)}\right)^n \left(\prod_{i=1}^n x_i\right)^{\alpha-1}
e^{-\beta\sum_{k=1}^n x_i}.
\\
\end{aligned}.
\]</span></p>
</div>


<div class="example">
<span id="exm:unnamed-chunk-85" class="example"><strong>Example 4.2  (Independent but not identically distributed Gamma)  </strong></span>If we now change the family of models <span class="math inline">\(\{\mathcal{P_\theta}\}_{\theta\in\Theta}\)</span> of the previous example
and assume that the entries of <span class="math inline">\(\bar{X}\)</span> are still independent but <strong>not</strong>
identically distributed, for example if we assume that <span class="math inline">\(X_i\)</span> has distribution
<span class="math inline">\(Gamma(\alpha_i, \beta)\)</span> (they share the parameter <span class="math inline">\(\beta\)</span> but not necessarily
the parameter <span class="math inline">\(\alpha\)</span>), then we obtain
<span class="math display">\[
\begin{aligned}
L(\bar{x}\vert \alpha,\beta) &amp;= \prod_{i=1}^n L(x_i\vert \alpha_i,\beta) \\
&amp;= \frac{\beta^{\sum_{i=1}^n \alpha_i}}{\prod_{i=1}^n\Gamma(\alpha_i)} 
    \left(\prod_{i=1}^n x_i^{\alpha_i-1}\right) e^{-\beta \sum_{i=1}^n x_i}
\\
\end{aligned}.
\]</span>
</div>


<div class="example">
<span id="exm:unnamed-chunk-86" class="example"><strong>Example 4.3  (Not independent nor indentically distributed Gamma )  </strong></span>Let <span class="math inline">\(Y_1,Y_2\)</span> be independent and exponentially distributed random variables of parameter <span class="math inline">\(\lambda&gt;0\)</span>. Define
the vector
<span class="math display">\[
(X_1,X_2) = (Y_1, Y_1+Y_2).
\]</span>
Then <span class="math inline">\(X_1\)</span> is <span class="math inline">\(Gamma(1,\lambda)\)</span>-distributed, <span class="math inline">\(X_2\)</span> is <span class="math inline">\(Gamma(2,\lambda)\)</span>-distributed, and clearly <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are not
independent. It can be seen that the density function <span class="math inline">\(f_{(X_1,X_2)}\)</span> is given by
<span class="math display">\[
f_{(X_1,X_2)}(x_1,x_2) = \begin{cases}
 \left(\lambda e^{-\lambda x_1}\right) \left(\lambda e^{-\lambda(x_2-x_1)}\right) &amp; \text{ if } x_2\geq x_1\\
  0 &amp; \text{ otherwise},
\end{cases}
\]</span>
which cannot be seen as the product of two Gamma densities in <span class="math inline">\(\mathbb{R}\)</span> (due to the condition <span class="math inline">\(x_2\geq x_1\)</span>).
</div>

</div>
<div id="maximum-likelihood-estimator-mle-point-estimate" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Maximum Likelihood Estimator MLE (Point Estimate)</h3>
<p>The likelihood functions can be used to obtain a <strong>point estimate</strong> that
typically has very desirable properties.</p>

<div class="definition">
<span id="def:unnamed-chunk-87" class="definition"><strong>Definition 4.2  (Maximum Likelihood Estimator)  </strong></span>Let <span class="math inline">\(\{\mathcal{P}_\theta\}_{\theta\in\Theta}\)</span> be a family of probabilistic
models. Let <span class="math inline">\(\bar{x}=(x_1,\dots,x_n)\)</span> be a sample (a set of values). The
<strong>maximum likelihood</strong> estimator for <span class="math inline">\(\theta\)</span> based on the sample <span class="math inline">\(\bar{x}\)</span>
is given by
<span class="math display">\[
\hat{\theta}(\bar{x}) = arg\sup_{\theta\in\Theta} L(\bar{x}\vert \theta),
\]</span>
it can be interpreted as the choice of <span class="math inline">\(\theta\)</span> under which
the data <span class="math inline">\(\bar{x}\)</span> is better explained
by <span class="math inline">\(\mathcal{P}_{\theta}\)</span>, where the
<strong>criterion</strong> used to compare models, i.e.Â to say that a model <span class="math inline">\(\mathcal{P}_{\theta_1}\)</span>
âexplains the data betterâ than the model <span class="math inline">\(\mathcal{P}_{\theta_2}\)</span>,
is simply that of saying that it is more likely to observe the
data <span class="math inline">\(\bar{x}\)</span> under the model <span class="math inline">\(\mathcal{P}_{\theta_1}\)</span> than under the model <span class="math inline">\(\mathcal{P}_{\theta_2}\)</span>.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> The definition of the MLE of course involves a maximization problem. In the simple cases this maximization problem
can be solved easily through standard calculus techniques such as finding critical points by solving the equation
<span class="math display">\[
\frac{d}{d\theta}L(\bar{x}\vert \theta) = 0
\]</span>
and then using the second derivative criterion, or some other criterion, to ensure that the point found is a maximum.
However, in some other cases this may not be achievable so that one ends up using numerical maximization techniques.
</div>


<div class="example">
<p><span id="exm:normalMuMLE" class="example"><strong>Example 4.4  (Normal MLE for <span class="math inline">\(\mu\)</span>)  </strong></span>Let <span class="math inline">\(\bar{X}=(X_1,\dots,X_n)\)</span> be i.i.d <span class="math inline">\(normal(\mu,1)\)</span>. Then,
for any sample point <span class="math inline">\(\bar{x}\)</span>, the likelihood function based on <span class="math inline">\(\bar{x}\)</span> is given by</p>
<p><span class="math display">\[
L(\bar{x}\vert \mu) = \frac{1}{(2\pi)^{n/2}} 
e^{-\frac{1}{2}\sum_{i=1}^n(x_i-\mu)^2}.
\]</span></p>
The equation
<span class="math display">\[
\frac{d}{d\mu}L(\bar{x} \vert \mu) = 0
\]</span>
reduces to
<span class="math display">\[
\sum_{k=1}^n(x_k-\mu) = 0
\]</span>
which has solution
<span class="math display">\[
\hat{\mu}(\bar{x}) = \frac{1}{n} \sum_{k=1}^N x_k = \tilde{x}.
\]</span>
Yay! We have found that the MLE estimator for the population mean <span class="math inline">\(\mu\)</span> is simply the sample mean, i.e.Â that
<span class="math inline">\(\hat{\mu}=\tilde{X}\)</span>, which really aligns well with the intuitive approach of estimating the theoretical
mean by the âexperimantelly observedâ mean. In fact we have gained a lot more, we have developed a
solid theoretical framework which can be used to estimate any paremeter of any distribution and have found
that, at least in this case, it outputs very sensible results. As discussed further below in some cases we may have even
gained the ability to study the error and/or variance of our estimation.
</div>

<p><br />
</p>
<p>For illustration purposes we first show the graph of <span class="math inline">\(L(\bar{x}\vert \mu)\)</span> for a single simulated value of
<span class="math inline">\(\bar{x}\)</span> with <span class="math inline">\(Normal(\mu=1, \sigma=1)\)</span> distribution
<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>
along with the resulting value of the MLE <span class="math inline">\(\hat{\mu}(\bar{x})\)</span> and compare it with the true value of <span class="math inline">\(\mu=1\)</span>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" title="1"><span class="co">## Create (vectorized) likelihood function</span></a>
<a class="sourceLine" id="cb4-2" title="2">lnorm &lt;-<span class="st"> </span><span class="kw">Vectorize</span>( <span class="dt">FUN =</span> <span class="cf">function</span>(data, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>){</a>
<a class="sourceLine" id="cb4-3" title="3">                            <span class="kw">return</span>(<span class="kw">prod</span>(<span class="kw">dnorm</span>(data, <span class="dt">mean =</span> mean, <span class="dt">sd =</span> sd)))</a>
<a class="sourceLine" id="cb4-4" title="4">                          },</a>
<a class="sourceLine" id="cb4-5" title="5">                    <span class="dt">vectorize.args =</span> <span class="kw">c</span>(<span class="st">&quot;mean&quot;</span>, <span class="st">&quot;sd&quot;</span>))</a>
<a class="sourceLine" id="cb4-6" title="6"></a>
<a class="sourceLine" id="cb4-7" title="7"><span class="co"># Generate random sample </span></a>
<a class="sourceLine" id="cb4-8" title="8">data &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="co"># mu = 1 is the `real&#39; parameter</span></a>
<a class="sourceLine" id="cb4-9" title="9">mle &lt;-<span class="st"> </span><span class="kw">mean</span>(data)</a>
<a class="sourceLine" id="cb4-10" title="10"></a>
<a class="sourceLine" id="cb4-11" title="11"><span class="co"># Visualize likelihoods under different mus</span></a>
<a class="sourceLine" id="cb4-12" title="12">mus &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.5</span>, <span class="fl">1.5</span>, <span class="dt">length.out =</span> <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb4-13" title="13">lmus &lt;-<span class="st"> </span><span class="kw">lnorm</span>(data, <span class="dt">mean =</span> mus)</a>
<a class="sourceLine" id="cb4-14" title="14"><span class="kw">plot</span>(<span class="dt">x =</span> mus, <span class="dt">y =</span> lmus, <span class="dt">xlab =</span> <span class="kw">TeX</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">mu$&quot;</span>), <span class="dt">ylab =</span> <span class="kw">TeX</span>(<span class="st">&quot;$L(</span><span class="ch">\\</span><span class="st">bar{x} | </span><span class="ch">\\</span><span class="st">mu)$&quot;</span>), <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,</a>
<a class="sourceLine" id="cb4-15" title="15">     <span class="dt">main =</span> <span class="kw">TeX</span>(<span class="st">&quot;Likelihood function&quot;</span>))</a>
<a class="sourceLine" id="cb4-16" title="16"><span class="kw">abline</span>(<span class="dt">v =</span> <span class="dv">1</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb4-17" title="17"><span class="kw">abline</span>(<span class="dt">v =</span> mle, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span> )</a>
<a class="sourceLine" id="cb4-18" title="18"><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Likelihood&quot;</span>, <span class="st">&quot;MLE&quot;</span>, <span class="st">&quot;Truth&quot;</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-89-1.png" width="672" /></p>
<p>We now compare the distribution estimated through the MLE (<span class="math inline">\(\hat{\mu}\)</span>) against the
observed (empirical) distribution of the data and the true density (<span class="math inline">\(\mu=1\)</span>).</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" title="1"><span class="co"># Visualize estimated density VS normal densities under different mus</span></a>
<a class="sourceLine" id="cb5-2" title="2"><span class="kw">plot</span>(<span class="kw">density</span>(data), <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;density&quot;</span>, </a>
<a class="sourceLine" id="cb5-3" title="3">     <span class="dt">main =</span> <span class="st">&quot;Density Comparison&quot;</span>)</a>
<a class="sourceLine" id="cb5-4" title="4">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">4</span>,<span class="dt">length.out =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb5-5" title="5">mus &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">length.out =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb5-6" title="6">ignore &lt;-<span class="st"> </span><span class="kw">mapply</span>(lines, <span class="dt">y =</span> <span class="kw">lapply</span>(mus, dnorm, <span class="dt">x =</span> x, <span class="dt">sd =</span> <span class="dv">1</span>), </a>
<a class="sourceLine" id="cb5-7" title="7">       <span class="dt">MoreArgs =</span> <span class="kw">list</span>(<span class="st">&#39;x&#39;</span> =<span class="st"> </span>x, <span class="st">&#39;col&#39;</span> =<span class="st"> &quot;gray&quot;</span>, <span class="st">&#39;lwd&#39;</span> =<span class="st"> </span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb5-8" title="8"><span class="kw">abline</span>(<span class="dt">v =</span> mus, <span class="dt">lwd =</span> <span class="dv">1</span>, <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>)</a>
<a class="sourceLine" id="cb5-9" title="9"><span class="kw">lines</span>(x, <span class="kw">dnorm</span>(x, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">1</span>), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb5-10" title="10"><span class="kw">abline</span>(<span class="dt">v =</span> <span class="dv">1</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb5-11" title="11"><span class="kw">lines</span>(x, <span class="kw">dnorm</span>(x, <span class="dt">mean =</span> mle, <span class="dt">sd =</span> <span class="dv">1</span>), <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb5-12" title="12"><span class="kw">abline</span>(<span class="dt">v =</span> mle, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>)</a>
<a class="sourceLine" id="cb5-13" title="13"><span class="kw">legend</span>(<span class="st">&quot;left&quot;</span>, <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;data&quot;</span>, <span class="st">&quot;mle&quot;</span>, <span class="st">&quot;truth&quot;</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">lwd =</span> <span class="dv">2</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-90-1.png" width="672" /></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" title="1"><span class="co"># Visualize estimated cdf VS normal cdfs under different mus</span></a>
<a class="sourceLine" id="cb6-2" title="2">datacdf &lt;-<span class="st"> </span><span class="kw">ecdf</span>(data)</a>
<a class="sourceLine" id="cb6-3" title="3">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dt">length.out =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb6-4" title="4"><span class="kw">plot</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> <span class="kw">datacdf</span>(x), <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>), <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;CDF&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;CDF comparison&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb6-5" title="5">mus &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">length.out =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb6-6" title="6">ignore &lt;-<span class="st"> </span><span class="kw">mapply</span>(lines, <span class="dt">y =</span> <span class="kw">lapply</span>(mus, pnorm, <span class="dt">q =</span> x, <span class="dt">sd =</span> <span class="dv">1</span>), </a>
<a class="sourceLine" id="cb6-7" title="7">       <span class="dt">MoreArgs =</span> <span class="kw">list</span>(<span class="st">&#39;x&#39;</span> =<span class="st"> </span>x, <span class="st">&#39;col&#39;</span> =<span class="st"> &quot;gray&quot;</span>, <span class="st">&#39;lwd&#39;</span> =<span class="st"> </span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb6-8" title="8"><span class="kw">lines</span>(x, <span class="kw">pnorm</span>(x, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">1</span>), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb6-9" title="9"><span class="kw">lines</span>(x, <span class="kw">pnorm</span>(x, <span class="dt">mean =</span> mle, <span class="dt">sd =</span> <span class="dv">1</span>), <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb6-10" title="10"><span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">box.col =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;data&quot;</span>, <span class="st">&quot;mle&quot;</span>, <span class="st">&quot;truth&quot;</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">lwd =</span> <span class="dv">2</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-90-2.png" width="672" /></p>
<p>As discussed in Section <a href="introduction-to-statistics.html#bigpicture">4.1</a> the value <span class="math inline">\(\bar{x}\)</span> is typically equated with the experimentally observed data.
However, in the probabilistic world <span class="math inline">\(\bar{x}\)</span> is interpreted as just a ârealizationâ of the random vector <span class="math inline">\(\bar{X}\)</span>.
Now observe that <span class="math inline">\(\hat{\theta}(\bar{x})\)</span>, the MLE for <span class="math inline">\(\theta\)</span>, is a function of <span class="math inline">\(\bar{x}\)</span>,
and if we now think of <span class="math inline">\(\bar{x}\)</span> as a realization
of <span class="math inline">\(\bar{X}\)</span>, then in fact we may consider the random variable <span class="math inline">\(\hat{\theta}(\bar{X})\)</span>. Now, what does this mean
in our statistical framework? Well, if a particular value <span class="math inline">\(\bar{x}\)</span> is equated with the experimentally observed data, then we
can think of the experiment as a probabilistic procedure that outputs a random value of <span class="math inline">\(\bar{x}\)</span> every
time it is performed, thus we think of the <strong>experiment as a sampling procedure of <span class="math inline">\(\bar{X}\)</span> under its true distribution</strong>.
Thus, the results of our estimation,
in particular the value of the MLE point estimate, can also be seen as random
(which makes sense
since in reality if we repeated the experiment we wouldnt expect to obtain the same conclusions but not necessarily
exactly the same results). The MLE
then becomes a random variable and as such we may study its probabilistic behaviour, for example may study
its variance <span class="math inline">\(\mathbb{Var}\left(\hat{\theta}(\bar{X})\right)\)</span> which gives us a measure of how much we expect our estimation to
vary if we repeated the experiment multiple times; and even more, we may study
the probability
that the MLE (or in general our statistical procedure) provides a good estimate for the real distribution of
<span class="math inline">\(\bar{X}\)</span>, or the probability that it fails to do so.
In the particular case of the MLE point estimate we may, for example, study the difference between <span class="math inline">\(\hat{\theta}(\bar{X})\)</span>
and the true value of <span class="math inline">\(\theta\)</span>.</p>

<div class="example">
<span id="exm:normalMuMLEdistribution" class="example"><strong>Example 4.5  (Normal MLE <span class="math inline">\(\hat{\mu}\)</span> distribution)  </strong></span>Recall the situation of Example <a href="introduction-to-statistics.html#exm:normalMuMLE">4.4</a>. Assume that the âtrueâ distribution of <span class="math inline">\(\bar{X}\)</span> is
normal with mean <span class="math inline">\(\mu\)</span> and variance 1 and recall that
<span class="math inline">\(\hat{\mu}(\bar{X})= \tilde{X}\)</span> so that (see Section <a href="common-distributions.html#normalDistribution">3.8</a>) <span class="math inline">\(\hat{\mu}(\bar{X})\)</span> is again normally distributed with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(1/n\)</span>. The fact that the variance is <span class="math inline">\(1/n\)</span> is really cool since it means that as <span class="math inline">\(n\to\infty\)</span>,
i.e.Â as we gather more data, the estimated value <span class="math inline">\(\hat{\mu}\)</span> converges to the true value <span class="math inline">\(\mu\)</span>.
</div>

<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" title="1"><span class="co"># Generate 1000 random samples of size 100 each</span></a>
<a class="sourceLine" id="cb7-2" title="2">mu &lt;-<span class="st"> </span><span class="dv">1</span> <span class="co"># set a true value of the parameter</span></a>
<a class="sourceLine" id="cb7-3" title="3">datas &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">10000</span><span class="op">*</span><span class="dv">100</span>, <span class="dt">mean =</span> mu, <span class="dt">sd =</span> <span class="dv">1</span>), <span class="dt">nrow =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb7-4" title="4"><span class="co"># Compute MLE for each sample of size 100</span></a>
<a class="sourceLine" id="cb7-5" title="5">mles &lt;-<span class="st"> </span><span class="kw">colMeans</span>(datas)</a>
<a class="sourceLine" id="cb7-6" title="6"><span class="co"># Visualize MLE density and compare with true value mu0</span></a>
<a class="sourceLine" id="cb7-7" title="7"><span class="co"># Estimated density</span></a>
<a class="sourceLine" id="cb7-8" title="8"><span class="kw">plot</span>(<span class="kw">density</span>(mles), <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">xlab =</span> <span class="kw">TeX</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">hat{</span><span class="ch">\\</span><span class="st">mu}(</span><span class="ch">\\</span><span class="st">bar{x})$&quot;</span>), <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>, <span class="dt">main =</span> <span class="kw">TeX</span>(<span class="st">&quot;MLE ($</span><span class="ch">\\</span><span class="st">hat{</span><span class="ch">\\</span><span class="st">mu}(</span><span class="ch">\\</span><span class="st">bar{X})$) density&quot;</span>))</a>
<a class="sourceLine" id="cb7-9" title="9"><span class="co"># True density</span></a>
<a class="sourceLine" id="cb7-10" title="10">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="fl">0.7</span>, <span class="fl">1.5</span>, <span class="dt">length.out =</span> <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb7-11" title="11"><span class="kw">lines</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> <span class="kw">dnorm</span>(x, <span class="dt">mean =</span> mu, <span class="dt">sd =</span> <span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">100</span>)), <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb7-12" title="12"><span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;MLE sample density&quot;</span>, <span class="st">&quot;MLE true density&quot;</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>)</a>
<a class="sourceLine" id="cb7-13" title="13"><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">mean</span>(mles), <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb7-14" title="14"><span class="kw">abline</span>(<span class="dt">v =</span> mu, <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-91-1.png" width="672" />
<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>Thus, for example, we may compute the probability that
<span class="math inline">\(\left\lvert \hat{\mu}(\bar{X}) - \mu \right\rvert&gt; a\)</span> <strong>given</strong> that <span class="math inline">\(\bar{X}\overset{i.i.d}\sim normal(\mu,1)\)</span>
(i.e.Â under the model <span class="math inline">\(\mathcal{P}_\mu\)</span>), we have</p>
<p><span class="math display">\[\begin{align}
\mathbb{P}_\mu\left(\left\lvert \hat{\mu}(\bar{X}) - \mu \right\rvert&gt; a\right) &amp;= 
\int_{\mu-a}^{\mu+a} \sqrt{\frac{n}{2\pi}} 
                   e^{-\frac{n(x - \mu )^2}{2}} dx \\
&amp;= \int_{-a}^a \sqrt{\frac{n}{2\pi}} e^{-\frac{n x^2}{2}}dx;
\end{align}\]</span>
observe that in this case the distribution of the distance between the estimator <span class="math inline">\(\hat{\mu}(\bar{X})\)</span> and <span class="math inline">\(\mu\)</span>
under <span class="math inline">\(\mathcal{P}_{\mu}\)</span> has distribution <span class="math inline">\(normal(0,1/n)\)</span> which does not depend on <span class="math inline">\(\mu\)</span> so that we can
assure, irrespectively of the true value of <span class="math inline">\(\mu\)</span>, that the distance of our estimator <span class="math inline">\(\hat{\mu}(\bar{x})\)</span>
and <span class="math inline">\(\mu\)</span> will always be less than <span class="math inline">\(a\)</span> with probability
<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p><span class="math display">\[
\int_{-a}^a \sqrt{\frac{n}{2\pi}} e^{-\frac{n x^2}{2}}dx.
\]</span></p>
</div>
</div>
<div id="hypothesis-testing" class="section level2">
<h2><span class="header-section-number">4.3</span> Hypothesis testing</h2>
<div id="definition-and-evaluation" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Definition and Evaluation</h3>

<div class="definition">
<span id="def:unnamed-chunk-92" class="definition"><strong>Definition 4.3  </strong></span>A <strong>hypothesis</strong> is a statement about a population/model parameter.
</div>

<p>The goal of a hypothesis test is to decide, based on a sample from the population, which of two complementary hypothesis is true.</p>

<div class="definition">
<span id="def:unnamed-chunk-93" class="definition"><strong>Definition 4.4  </strong></span>The two complementary hypothesis in a hypothesis testing problem are called
the <strong>null Hypothesis (<span class="math inline">\(H_0\)</span>)</strong> and the <strong>alternative hypothesis (<span class="math inline">\(H_1\)</span>)</strong>.
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-94" class="definition"><strong>Definition 4.5  (Hypothesis Test)  </strong></span>A <strong>hypothesis testing procedure</strong> is a rule that sepecifies:</p>
<ol style="list-style-type: decimal">
<li>For which sample values <span class="math inline">\(\bar{x}\in\mathbb{R}^n\)</span> the decision is made to accept <span class="math inline">\(H_0\)</span> as true.</li>
<li>For which sample values <span class="math inline">\(\bar{x}\in\mathbb{R}^n\)</span> the decision is made to reject <span class="math inline">\(H_0\)</span> (âin acceptance of <span class="math inline">\(H_1\)</span>â).</li>
</ol>
The region <span class="math inline">\(R\subset\mathbb{R}^n\)</span> of the sample space on which <span class="math inline">\(H_0\)</span> is rejected is called the <strong>rejection region</strong> <span class="math inline">\(R\)</span>, while the region in which <span class="math inline">\(H_0\)</span> is accepted is called the
<strong>acceptance region</strong>.
</div>

<p><br />
</p>
<p>Typically a hypothesis test is specified in terms of a <strong>test statistic</strong> <span class="math inline">\(W(X_1,\dots,X_n)\)</span>, i.e.Â a function
<span class="math inline">\(W\colon \mathbb{R}^n\to\mathbb{R}\)</span> of the sample <span class="math inline">\(\bar{X}=(X_1,\dots,X_n)\)</span>.</p>

<div class="example">
<p><span id="exm:normMLEtest" class="example"><strong>Example 4.6  (Normal <span class="math inline">\(\sigma\)</span> known)  </strong></span>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be an i.i.d sample of a <span class="math inline">\(normal(\mu,\sigma)\)</span> distribution with unknown <span class="math inline">\(\mu\)</span> and known <span class="math inline">\(\sigma\)</span>.
Recall from Example <a href="introduction-to-statistics.html#exm:normalMuMLE">4.4</a> that the MLE for <span class="math inline">\(\mu\)</span> is
<span class="math display">\[
\hat{\mu}(\bar{X})=\tilde{X}.
\]</span>
Fix a value <span class="math inline">\(\mu_0\)</span>. We may define a decision rule for testing <span class="math inline">\(H_0\colon\mu=\mu_0\)</span> versus <span class="math inline">\(H_1\colon\mu\neq \mu_0\)</span>
using the statistic <span class="math inline">\(\hat{\mu}(\bar{X})\)</span>. To do so it would be reasonable to consider a rejection region of the form</p>
<p><span class="math display">\[
\begin{aligned}
R&amp;=\{\hat{x}\in\mathbb{R}^n\colon\left\lvert \hat{\mu}(\hat{x}) - \mu_0 \right\rvert &gt; c_n\}\\
&amp;=\{\hat{x}\in\mathbb{R}^n\colon\left\lvert \tilde{x} - \mu_0 \right\rvert &gt; c_n\}\subset \mathbb{R}^n.
\end{aligned}
\]</span></p>
for some prespecified value of <span class="math inline">\(c_n\)</span>
<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>, <a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>.
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-95" class="definition"><strong>Definition 4.6  (Type I and II Errors)  </strong></span>A hypothesis test for <span class="math inline">\(H_0\colon \theta\in\Theta_0\)</span> VS <span class="math inline">\(H_1\colon \theta\not\in\Theta_0\)</span> can make two types of errors</p>
<ul>
<li><strong>Type I error</strong>: When <span class="math inline">\(H_0\colon\theta\in\Theta_0\)</span> is true but the test decides that <span class="math inline">\(H_0\)</span> is false (false positive).</li>
<li><strong>Type II error</strong>: When <span class="math inline">\(H_1\colon\theta\not\in\Theta_0\)</span> is true but the test decides that <span class="math inline">\(H_0\)</span> is true (false negative).</li>
</ul>
<table>
<thead>
<tr class="header">
<th></th>
<th></th>
<th>Decision</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td>Accept H_0</td>
<td>Reject H_0</td>
</tr>
<tr class="even">
<td><strong>Truth</strong></td>
<td>H_0</td>
<td>Correct Decision</td>
<td>Type I error</td>
</tr>
<tr class="odd">
<td></td>
<td>H_1</td>
<td>Type II error</td>
<td>Correct Decision</td>
</tr>
</tbody>
</table>
</div>

<p>Now let <span class="math inline">\(\theta\in\Theta_0\)</span> and assume <span class="math inline">\(\bar{X}\sim \mathcal{P}_\theta\)</span>
(i.e.Â assume <span class="math inline">\(\theta\)</span> is the true value of the parameter) so that the test will make a mistake, a Type I error,
if <span class="math inline">\(\bar{X}\in R\)</span> which occurs with probability
<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>
<p><span class="math display">\[\mathbb{P}_\theta(\bar{X}\in R).\]</span>
Alternatively, if <span class="math inline">\(\theta\in \Theta_0^c\)</span> and we assume <span class="math inline">\(\bar{X}\sim \mathcal{P}_\theta\)</span> then
the test will make a mistake, a Type II error, if <span class="math inline">\(\bar{X}\in R^c\)</span> which occurs with probability
<span class="math display">\[1-\mathbb{P}_\theta(\bar{X}\in R).\]</span>
Observe then that the function
<span class="math display">\[\theta\mapsto \mathbb{P}_\theta(\bar{X}\in R)\]</span>
summarizes all the information of the test with rejection region <span class="math inline">\(R\)</span>, in particular summarizes the probabilities
of making Type I and II errors, thus calling for the following definition.</p>

<div class="definition">
<span id="def:unnamed-chunk-96" class="definition"><strong>Definition 4.7  (Power Function)  </strong></span>The <strong>power function</strong> of a hypothesis test with rejection region <span class="math inline">\(R\subset \mathbb{R}^n\)</span> is the function of <span class="math inline">\(\theta\)</span> defined by
<span class="math display">\[
\beta(\theta):=\mathbb{P}_\theta(X\in\ R).
\]</span>
</div>

<p>The ideal power function is <span class="math inline">\(0\)</span> for all <span class="math inline">\(\theta\in\Theta_0\)</span>, which gives that Type I errors occur with zero probability if
the true value of <span class="math inline">\(\theta\)</span> is in <span class="math inline">\(\Theta_0\)</span>, and <span class="math inline">\(1\)</span> for all <span class="math inline">\(\theta\in\Theta_0^c\)</span>, which similarly gives that
Type II errors occur with zero probability whenever the true value of <span class="math inline">\(\theta\)</span> is in <span class="math inline">\(\Theta_0^c\)</span>.</p>

<div class="example">
<p><span id="exm:twosidedNormal" class="example"><strong>Example 4.7  </strong></span>Recall the test <span class="math inline">\(H_0\colon\mu=\mu_0\)</span> VS <span class="math inline">\(H_1\colon\mu\neq\mu_0\)</span> of Example <a href="introduction-to-statistics.html#exm:normMLEtest">4.6</a> with rejection region
<span class="math display">\[
R=\{\hat{x}\in\mathbb{R}^n\colon\left\lvert \tilde{x} - \mu_0 \right\rvert &gt; c_n\}.
\]</span>
Observe that the test will make a Type I error if <span class="math inline">\(\mu_0\)</span> is the true value of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\left\lvert \tilde{X} - \mu_0 \right\rvert &gt; c_n\)</span>,
which occurs with probability (see Example <a href="introduction-to-statistics.html#exm:normalMuMLEdistribution">4.5</a>)
<span class="math display">\[\mathbb{P}_{\mu_0}(\left\lvert \tilde{X} - \mu_0 \right\rvert &gt; c_n)= \int_{\mu_0-c_n}^{\mu_0+c_n} \sqrt{\frac{n}{2\pi}}
                   e^{-\frac{n(x - \mu_0 )^2}{2}} dx.\]</span>
On the other hand, the test will make a Type II error if <span class="math inline">\(\mu_1\neq\mu_0\)</span> is the true value of <span class="math inline">\(\mu\)</span> and
<span class="math inline">\(\left\lvert \tilde{X} - \mu_0 \right\rvert \leq c_n\)</span>, which occurs with probability
<span class="math display">\[
\begin{aligned}
\mathbb{P}_{\mu_1}(\left\lvert \tilde{X} - \mu_0 \right\rvert \leq c_n)&amp;= 1-\mathbb{P}_{\mu_1}(\left\lvert \tilde{X} - \mu_0 \right\rvert &gt; c_n)\\
&amp;=1-\int_{\mu_0-c_n}^{\mu_0+c_n} \sqrt{\frac{n}{2\pi}}
                   e^{-\frac{n(x - \mu_1 )^2}{2}} dx.
\end{aligned}
\]</span>
The power function of this test is
<span class="math display">\[
\begin{aligned}
\beta(\mu) = \mathbb{P}_{\mu}(\left\lvert \tilde{X} - \mu_0 \right\rvert &gt; c_n)= \int_{\mu_0-c_n}^{\mu_0+c_n} \sqrt{\frac{n}{2\pi}}
                   e^{-\frac{n(x - \mu)^2}{2}} dx.
\end{aligned}
\]</span>
Now assume we choose <span class="math inline">\(c_n=c\sigma\sqrt{1/n}\)</span> for some <span class="math inline">\(c&gt;0\)</span> <a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> so that, rearranging terms, the rejection region becomes</p>
<p><span class="math display" id="eq:normalTwoSidedRR">\[\begin{align}
R
&amp;=\left\{\hat{x}\in\mathbb{R}^n\colon\left\lvert \frac{\tilde{x} - \mu_0}{\sigma/\sqrt{n}} \right\rvert &gt; c\right\}\tag{4.1}\\
&amp;=\left\{\hat{x}\in\mathbb{R}^n\colon \frac{\tilde{x} - \mu}{\sigma/\sqrt{n}} &lt; - c + \frac{\mu_0 - \mu}{\sigma/\sqrt{n}} \right\}\cup 
  \left\{\hat{x}\in\mathbb{R}^n\colon \frac{\tilde{x} - \mu}{\sigma/\sqrt{n}} &gt; c + \frac{\mu_0 - \mu}{\sigma/\sqrt{n}}\right\}.
\end{align}\]</span></p>
Observe that when the true value of the parameter is <span class="math inline">\(\mu\)</span>,
i.e.Â under the assumption <span class="math inline">\(\bar{X}\overset{i.i.d}{\sim}normal(\mu,\sigma)\)</span>,
we have that the random variable
<span class="math display">\[
Z:=\frac{\tilde{x} - \mu}{\sigma/\sqrt{n}}
\]</span>
(often called the <em>Z-statistic</em>) is <span class="math inline">\(normal(0,1)\)</span> distributed, so that the power function becomes
<span class="math display">\[
\begin{aligned}
\beta(\mu)&amp;=\mathbb{P}_\mu(\bar{X}\in R) \\
&amp;=\mathbb{P}\left( Z \in \left[\frac{\mu_0 - \mu}{\sigma/\sqrt{n}} -c, \frac{\mu_0 - \mu}{\sigma/\sqrt{n}} +c\right]^c\right) \\
&amp;=\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\frac{\mu_0 - \mu}{\sigma/\sqrt{n}} -c} e^{-\frac{x^2}{2}}dx +
 \frac{1}{\sqrt{2\pi}} \int_{\frac{\mu_0 - \mu}{\sigma/\sqrt{n}} +c}^\infty e^{-\frac{x^2}{2}}dx .
\end{aligned}
\]</span>
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-97" class="definition"><strong>Definition 4.8  </strong></span><br />
</p>
<ul>
<li><p>For <span class="math inline">\(0\leq\alpha\leq 1\)</span>, a test with power function <span class="math inline">\(\beta(\theta)\)</span> is a <strong>size <span class="math inline">\(\alpha\)</span> test</strong> if
<span class="math display">\[
\sup_{\theta\in\Theta_0} \beta(\theta)= \alpha.
\]</span></p></li>
<li>For <span class="math inline">\(0\leq\alpha\leq 1\)</span>, a test with power function <span class="math inline">\(\beta(\theta)\)</span> is a <strong>level <span class="math inline">\(\alpha\)</span> test</strong> if
<span class="math display">\[
\sup_{\theta\in\Theta_0} \beta(\theta)\leq \alpha.
\]</span>
</div></li>
</ul>

<div class="example">
<span id="exm:unnamed-chunk-98" class="example"><strong>Example 4.8  </strong></span>We now consider a hypothesis test of the form <span class="math inline">\(H_0\colon\mu\leq \mu_0\)</span> VS <span class="math inline">\(H_1\colon\mu&gt;\mu_0\)</span> for
a sample <span class="math inline">\(\bar{X}\)</span> with <span class="math inline">\(normal(\mu,\sigma)\)</span> distribution (<span class="math inline">\(\sigma\)</span> known). Again it is reasonable
to construct the rejection region <span class="math inline">\(R\)</span> as
<span class="math display">\[
\begin{aligned}
R&amp;=\left\{\hat{x}\in\mathbb{R}^n\colon \frac{\tilde{x} - \mu_0}{\sigma/\sqrt{n}} &gt; c\right\}\\
 &amp;=\left\{\hat{x}\in\mathbb{R}^n\colon \frac{\tilde{x} - \mu}{\sigma/\sqrt{n}} &gt; c + \frac{\mu_0 - \mu}{\sigma/\sqrt{n}}\right\}
\end{aligned}
\]</span>
for some <span class="math inline">\(c&gt;0\)</span>. Letting
<span class="math display">\[
Z :=\frac{\tilde{x} - \mu}{\sigma/\sqrt{n}}
\]</span> as in Example <a href="introduction-to-statistics.html#exm:twosidedNormal">4.7</a> we obtain that the power function <span class="math inline">\(\beta(\mu)\)</span> is given by
<span class="math display">\[
\beta(\mu) = \mathbb{P}\left(Z &gt;  c + \frac{\mu_0 - \mu}{\sigma/\sqrt{n}}\right),
\]</span>
so that
<span class="math display">\[
\sup_{\mu\leq \mu_0} \beta(\mu) = \mathbb{P}\left(Z &gt;  c\right).
\]</span>
Thus, choosing <span class="math inline">\(c\)</span> so that <span class="math inline">\(\mathbb{P}\left(Z &gt; c\right)=\alpha\)</span>,
i.e.Â if we choose <span class="math inline">\(c\)</span> to be the <span class="math inline">\(\alpha\)</span>-quantile of the standard normal distribution <span class="math inline">\(z_{\alpha}\)</span>,
we obtain a size <span class="math inline">\(\alpha\)</span> test.
</div>

</div>
<div id="likelihood-ratio-tests-lrts" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Likelihood Ratio Tests (LRTs)</h3>
<p>In this section we introduced a general theoretical framework for the construction of the rejection region <span class="math inline">\(R\)</span> which
can be applied to any parameter <span class="math inline">\(\theta\)</span> of any model <span class="math inline">\(\mathcal{P}_\theta\)</span>.</p>

<div class="definition">
<span id="def:unnamed-chunk-99" class="definition"><strong>Definition 4.9  </strong></span>The <strong>likelihood ratio test (LRT) statistic</strong> for testing <span class="math inline">\(H_0\colon \theta\in\Theta_0\subset\Theta\)</span> versus
<span class="math inline">\(H_1\colon \theta \not\in \Theta_0\)</span> is defined as
<span class="math display">\[
\lambda(\bar{x})=\frac{\sup_{\theta\in\Theta_0} L(\theta\vert \bar{x})}{\sup_{\theta\in\Theta} L(\theta\vert\bar{x})}.
\]</span>
A <strong>likelihood ratio test based on <span class="math inline">\(\lambda(\bar{x})\)</span></strong> is any test that has a rejection region of the form
<span class="math display">\[
R=\{\bar{x}\colon \lambda(\bar{x})\leq c\},
\]</span>
where <span class="math inline">\(c\)</span> is a presepecified value <span class="math inline">\(0\leq c\leq 1\)</span>. Observe that we
always have <span class="math inline">\(0\leq \lambda(\bar{x})\leq 1\)</span>) and that values of <span class="math inline">\(\lambda(\bar{x})\)</span> that are close to <span class="math inline">\(0\)</span>
support rejection of <span class="math inline">\(H_0\)</span> while values close to <span class="math inline">\(1\)</span> support acceptance of <span class="math inline">\(H_0\)</span>.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> Again we encounter maximization problems in order to construct a LRTs. Observe that in the numerator we encounter
a <strong>constrained</strong> maximization problem since we need to maximize over the set <span class="math inline">\(\Theta_0\subset\Theta\)</span>, while
the maximization in the denominator is a global maximization problem.
</div>

<p><br />
</p>

<div class="example">
<p><span id="exm:unnamed-chunk-101" class="example"><strong>Example 4.9  </strong></span>Let <span class="math inline">\(\bar{X}=(X_1,\dots,X_n)\)</span> be an i.i.d sample with <span class="math inline">\(normal(\mu,1)\)</span> distribution. Consider testing
<span class="math inline">\(H_0\colon \theta=\theta_0\)</span> VS <span class="math inline">\(H_1\colon\theta\neq\theta_0\)</span> (<span class="math inline">\(\theta_0\)</span> is a number fixed by the experimenter
prior to the experiment). Then, recalling that the MLE for <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\hat{\mu}(\bar{X})=\tilde{X}\)</span>,
<span class="math display">\[
\lambda(\bar{x}) = \frac{(2\pi)^{-n/2}e^{-\sum_{i=1}^n \frac{(x_i-\theta_0)^2}{2}}}
                        {(2\pi)^{-n/2}e^{-\sum_{i=1}^n \frac{(x_i-\tilde{x})^2}{2}}},
\]</span>
and since
<span class="math display">\[
\sum_{i=1}^n (x_i-\theta_0)^2 = n(\tilde{x}-\theta_0)^2 +  \sum_{i=1}^n (x_i-\tilde{x})^2,
\]</span>
the LRT statistic becomes
<span class="math display">\[
\lambda(\bar{x}) = e^{-\frac{n(\tilde{x}-\theta_0)^2}{2}}.
\]</span>
A LRT rejects <span class="math inline">\(H_0\)</span> for small values of <span class="math inline">\(\lambda(\bar{x})\)</span>, so that it has a rejection region of the form</p>
<p><span class="math display">\[
\begin{aligned}
R&amp;=\{\bar{x}\colon \lambda(\bar{x})\leq c\}\\
 &amp;=\left\{\bar{x}\colon \lambda(\bar{x})\leq \sqrt{-2\log(c)/n}\right\}
\end{aligned}
\]</span></p>
for some <span class="math inline">\(c\in[0,1]\)</span>. Compare this with <a href="introduction-to-statistics.html#eq:normalTwoSidedRR">(4.1)</a> by observing that as <span class="math inline">\(c\)</span> varies along <span class="math inline">\([0,1]\)</span> the
term <span class="math inline">\(-2\log(c)\)</span> varies along <span class="math inline">\((0,\infty)\)</span>.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-102" class="example"><strong>Example 4.10  </strong></span>Let <span class="math inline">\(\bar{X}=(X_1,\dots,X_n)\)</span> be an i.i.d sample with <span class="math inline">\(normal(\mu,\sigma)\)</span> distribution where both <span class="math inline">\(\mu\)</span> and
<span class="math inline">\(\sigma\)</span> are unkown and that we are interested in testing <span class="math inline">\(H_0\colon \mu\leq\mu_0\)</span> VS <span class="math inline">\(H_1\colon\mu&gt;\mu_0\)</span> (so
that <span class="math inline">\(\sigma\)</span> is a nuisance parameter).
Then</p>
<span class="math display">\[\begin{aligned}
\lambda(\bar{x}) &amp;= 
\frac{\sup_{\mu\leq\mu_0, \sigma\geq 0} L(\mu,\sigma^2 \vert \bar{x})}
     {\sup_{\mu\in\mathbb{R}, \sigma\geq 0} L(\mu,\sigma^2 \vert \bar{x})}\\
&amp;=\frac{\sup_{\mu\leq\mu_0, \sigma\geq 0} L(\mu,\sigma^2 \vert \bar{x})}
     {L(\hat{\mu},\hat{\sigma}^2 \vert \bar{x})}
\end{aligned}\]</span>
where of course <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\sigma}^2=\frac{1}{n}\sum (x_i - \tilde{x})^2\)</span>
are the MLEs of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. In particular,
if <span class="math inline">\(\hat{\mu}\leq\mu_0\)</span> then
<span class="math display">\[
\sup_{\mu\leq\mu_0, \sigma\geq 0} L(\mu,\sigma^2 \vert \bar{x}) = L(\hat{\mu},\hat{\sigma}^2 \vert \bar{x}),
\]</span>
and if <span class="math inline">\(\hat{\mu}&gt;m_0\)</span> then the restricted maximum is <span class="math inline">\(L(\mu_0, \hat{\sigma}^2_0\vert \bar{x})\)</span> where
<span class="math inline">\(\hat{\sigma}^2_0= \sum (x_i-\mu_0)^2/n\)</span>,
so that
<span class="math display">\[
\lambda(\bar{x}) = \begin{cases}
1 &amp; \text{ if }\hat{\mu} \leq \mu_0\\
\frac{L(\mu_0, \hat{\sigma}^2_0\vert \bar{x})}{L(\hat{\mu},\hat{\sigma}^2 \vert \bar{x})} &amp; \text{ otherwise}.
\end{cases}
\]</span>
With some rearranging of the terms it can be shown that the test based on the statistic <span class="math inline">\(\lambda(\bar{X})\)</span> is
equivalent to a test based on the <span class="math inline">\(t\)</span>-statistic
<span class="math display">\[
t=\frac{\tilde{X}-\mu}{\sqrt{\hat{\sigma}^2/n}}.
\]</span>
which has Studentâs-t distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.
</div>


</div>
</div>
</div>
























































































<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p> Here you can think of the call to <span class="math inline">\(rnorm\)</span> as âperforming an experimentâ that outputs some data <span class="math inline">\(\bar{x}\)</span> under
the true distribution of <span class="math inline">\(\bar{X}\sim normal(1,1)\)</span>. Of course, in contrast with the typical scenario in real life,
in this case we do know the true distribution of the data since we are simulating it,
this is useful if one wants to study how well does the statistical procedure works, for example by running
the code multiple times (which you can think of as repeating the experiment multiple times) one can get a good estimate
of the expected distance between <span class="math inline">\(\hat{\mu}(\bar{x})\)</span> and the true parameter
that we used for the simulations, in this case <span class="math inline">\(\mu=1\)</span>.<a href="introduction-to-statistics.html#fnref4" class="footnote-back">â©</a></p></li>
<li id="fn5"><p> Here the intuition of thinking an experiment as a âmachineâ generating values of <span class="math inline">\(\bar{x}\)</span> under the
true distribution of <span class="math inline">\(\bar{X}\)</span> is equated in the code with a call to <span class="math inline">\(rnorm\)</span>. You can think of
the columns of the matrix <em>datas</em> as multiple repetitions of the experiment, each with a different output
value of <span class="math inline">\(\bar{x}\)</span>.<a href="introduction-to-statistics.html#fnref5" class="footnote-back">â©</a></p></li>
<li id="fn6"><p>This can be used to construct what are called <strong>âConfidence intervalsâ</strong> for <span class="math inline">\(\mu\)</span> which are of the form
<span class="math display">\[
\left[\hat{\mu}(\bar{X}) - a, \hat{\mu}(\bar{X}) + a\right];
\]</span>
observe that this interval captures the true parameter <span class="math inline">\(\mu\)</span> with probability
<span class="math display">\[
\int_{-a}^a \sqrt{\frac{n}{2\pi}} e^{-\frac{n x^2}{2}}dx.
\]</span><a href="introduction-to-statistics.html#fnref6" class="footnote-back">â©</a></p></li>
<li id="fn7"><p>Later on we will discuss good choices for <span class="math inline">\(c_n\)</span><a href="introduction-to-statistics.html#fnref7" class="footnote-back">â©</a></p></li>
<li id="fn8"><p>At this point this hypothesis test, or decision rule, is intuitively reasonable
but somehow arbitrary. Further ahead we will introduce Likelihood Ratio Tests which provide a general theoretical
framework for the construction of rejection regions / hypothesis tests / decision rules; turns out that the
decision rule of this example can be obtained as a LRT.<a href="introduction-to-statistics.html#fnref8" class="footnote-back">â©</a></p></li>
<li id="fn9"><p> Here we use the notation <span class="math inline">\(\mathbb{P}_\theta\)</span> for the distribution of <span class="math inline">\(\bar{X}\)</span> under the
assumption <span class="math inline">\(\bar{X}\sim\mathcal{P}_\theta\)</span>; i.e.Â assuming that the true model is <span class="math inline">\(\mathcal{P}_\theta\)</span>.
Here we keep the same letter <span class="math inline">\(\theta\)</span> since later on we will want to consider <span class="math inline">\(\theta\)</span> as a variable
(much in the same sense in which we consider <span class="math inline">\(\theta\)</span> to be a variable in the definition of the likelihood function
<span class="math inline">\(L(\bar{x}\vert \theta)\)</span>.<a href="introduction-to-statistics.html#fnref9" class="footnote-back">â©</a></p></li>
<li id="fn10"><p>Further below we will argument how to choose <span class="math inline">\(c\)</span><a href="introduction-to-statistics.html#fnref10" class="footnote-back">â©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="common-distributions.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
